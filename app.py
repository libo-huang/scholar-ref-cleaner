import streamlit as st
import bibtexparser
from bibtexparser.bwriter import BibTexWriter
from bibtexparser.bibdatabase import BibDatabase
import requests
from thefuzz import fuzz
import time
import docx
import io

# --- é¡µé¢é…ç½® ---
st.set_page_config(page_title="AIæ–‡çŒ®å¹»è§‰æ£€æµ‹ä¸æ¸…æ´—", page_icon="ğŸ§¹", layout="wide")

st.title("ğŸ§¹ Scholar Ref Cleaner (Based on Semantic Scholar)")
st.markdown("""
**ä¸“æ²» AI ç”Ÿæˆçš„â€œå¹»è§‰â€å‚è€ƒæ–‡çŒ®ã€‚** ä¸Šä¼  BibTeX æˆ– Word æ–‡ä»¶ï¼Œæœ¬å·¥å…·å°†è°ƒç”¨ Semantic Scholar å®˜æ–¹æ•°æ®åº“è¿›è¡Œæ ¸å¯¹ï¼š
1. **éªŒè¯çœŸä¼ª**ï¼šæ£€æµ‹æ–‡çŒ®æ˜¯å¦å­˜åœ¨ã€‚
2. **è‡ªåŠ¨ä¿®æ­£**ï¼šä¿®æ­£é”™è¯¯çš„å¹´ä»½ã€ä½œè€…å’ŒæœŸåˆŠåã€‚
3. **è¾“å‡ºæŠ¥å‘Š**ï¼šä¸‹è½½æ¸…æ´—åçš„å¹²å‡€æ•°æ®ã€‚
""")

# --- æ ¸å¿ƒå‡½æ•°ï¼šè°ƒç”¨ Semantic Scholar API ---
def search_semantic_scholar(query_title):
    """åœ¨ Semantic Scholar ä¸­æœç´¢è®ºæ–‡"""
    base_url = "https://api.semanticscholar.org/graph/v1/paper/search"
    params = {
        "query": query_title,
        "limit": 1,
        "fields": "title,authors,year,venue,externalIds,url,citationCount"
    }
    try:
        # æ³¨æ„ï¼šæœªç”³è¯· API Key é™åˆ¶æ¯ç§’ 1 æ¬¡è¯·æ±‚ï¼Œè¿™é‡ŒåŠ å»¶è¿Ÿé˜²æ­¢å°ç¦
        time.sleep(1.1) 
        response = requests.get(base_url, params=params, timeout=10)
        if response.status_code == 200:
            data = response.json()
            if data['total'] > 0:
                return data['data'][0]
    except Exception as e:
        return None
    return None

def process_bib_entry(entry):
    """å¤„ç†å•ä¸ª Bib æ¡ç›®"""
    original_title = entry.get('title', '').replace('{', '').replace('}', '').replace('\n', ' ')
    if not original_title:
        return entry, "è·³è¿‡ (æ— æ ‡é¢˜)", 0

    # æœç´¢
    paper = search_semantic_scholar(original_title)
    
    if not paper:
        entry['note'] = "âš ï¸ NOT FOUND / HALLUCINATION"
        return entry, "âŒ æœªæ‰¾åˆ° (å¯èƒ½æ˜¯å¹»è§‰)", 0

    # æ¯”å¯¹æ ‡é¢˜ç›¸ä¼¼åº¦
    real_title = paper.get('title', '')
    similarity = fuzz.ratio(original_title.lower(), real_title.lower())

    status = ""
    if similarity > 85:
        # åŒ¹é…æˆåŠŸï¼Œè¦†ç›–æ•°æ®
        entry['title'] = real_title
        entry['year'] = str(paper.get('year', entry.get('year', '')))
        
        # å¤„ç†ä½œè€…
        if 'authors' in paper and paper['authors']:
            author_list = [a['name'] for a in paper['authors']]
            entry['author'] = " and ".join(author_list)
        
        # å¤„ç†æœŸåˆŠ/ä¼šè®®
        if 'venue' in paper and paper['venue']:
            entry['journal'] = paper['venue']
        
        entry['note'] = "âœ… Verified"
        status = f"âœ… å·²ä¿®æ­£ (ç›¸ä¼¼åº¦ {similarity}%)"
    else:
        # ç›¸ä¼¼åº¦è¿‡ä½ï¼Œæ ‡è®°è­¦å‘Š
        entry['note'] = f"â“ Low Confidence (Match: {real_title})"
        status = f"âš ï¸ å­˜ç–‘ (æœåˆ°çš„æ ‡é¢˜å·®å¼‚å¤§: {real_title})"

    return entry, status, similarity

# --- ä¾§è¾¹æ ï¼šä½¿ç”¨è¯´æ˜ ---
with st.sidebar:
    st.header("ä½¿ç”¨æŒ‡å—")
    st.info("""
    1. **BibTeX æ¨¡å¼(æ¨è)**ï¼š
       ä¸Šä¼  .bib æ–‡ä»¶ï¼Œè¾“å‡ºæ ‡å‡†çš„ .bib æ–‡ä»¶ï¼Œå¯ç›´æ¥å¯¼å…¥ LaTeX/Zoteroã€‚
    2. **Word æ¨¡å¼(å®éªŒæ€§)**ï¼š
       ä¸Šä¼  .docx æ–‡ä»¶ï¼Œä»…è¯»å–å…¶ä¸­çš„æ–‡æœ¬è¡Œè¿›è¡Œæœç´¢ï¼Œè¾“å‡ºéªŒè¯ç»“æœæ–‡æœ¬ã€‚
    """)
    st.warning("æ³¨æ„ï¼šSemantic Scholar å…è´¹ç‰ˆ API é€Ÿåº¦è¾ƒæ…¢ï¼Œè¯·è€å¿ƒç­‰å¾…ã€‚")

# --- ä¸»ç•Œé¢é€»è¾‘ ---
tab1, tab2 = st.tabs(["BibTeX æ–‡ä»¶å¤„ç†", "Word/æ–‡æœ¬å¤„ç†"])

# === TAB 1: BibTeX å¤„ç† ===
with tab1:
    uploaded_file = st.file_uploader("ä¸Šä¼  .bib æ–‡ä»¶", type="bib")
    
    if uploaded_file is not None:
        # è¯»å–æ–‡ä»¶
        bib_database = bibtexparser.load(uploaded_file)
        st.write(f"è¯†åˆ«åˆ° {len(bib_database.entries)} æ¡æ–‡çŒ®ï¼Œç‚¹å‡»ä¸‹æ–¹æŒ‰é’®å¼€å§‹æ¸…æ´—ã€‚")
        
        if st.button("å¼€å§‹æ¸…æ´— (BibTeX)", key="btn_bib"):
            progress_bar = st.progress(0)
            log_area = st.empty()
            
            cleaned_entries = []
            results_data = []

            for i, entry in enumerate(bib_database.entries):
                # æ›´æ–°è¿›åº¦
                progress_bar.progress((i + 1) / len(bib_database.entries))
                
                # å¤„ç†
                title_preview = entry.get('title', 'Unknown')[:30] + "..."
                log_area.text(f"æ­£åœ¨å¤„ç† [{i+1}/{len(bib_database.entries)}]: {title_preview}")
                
                new_entry, status, score = process_bib_entry(entry)
                cleaned_entries.append(new_entry)
                
                results_data.append({
                    "åŸæ ‡é¢˜": entry.get('title'),
                    "çŠ¶æ€": status,
                    "ä¿®æ­£åå¹´ä»½": new_entry.get('year')
                })

            # å®Œæˆå¤„ç†
            st.success("æ¸…æ´—å®Œæˆï¼")
            
            # å±•ç¤ºç®€æŠ¥
            st.dataframe(results_data)
            
            # ç”Ÿæˆä¸‹è½½
            db = BibDatabase()
            db.entries = cleaned_entries
            writer = BibTexWriter()
            cleaned_bib_str = writer.write(db)
            
            st.download_button(
                label="ğŸ“¥ ä¸‹è½½æ¸…æ´—åçš„ .bib æ–‡ä»¶",
                data=cleaned_bib_str,
                file_name="cleaned_references.bib",
                mime="text/plain"
            )

# === TAB 2: Word/æ–‡æœ¬ å¤„ç† ===
with tab2:
    st.markdown("ç”±äº Word æ ¼å¼å¤æ‚ï¼Œæœ¬åŠŸèƒ½**ä»…è¯»å–æ–‡æ¡£ä¸­çš„æ¯ä¸€æ®µæ–‡å­—**ä½œä¸ºæ ‡é¢˜å»æœç´¢ï¼Œæ— æ³•ç›´æ¥ç”Ÿæˆå®Œç¾æ ¼å¼çš„ Wordï¼Œä½†å¯ä»¥å¸®ä½ **æ’æŸ¥å‡æ–‡çŒ®**ã€‚")
    uploaded_word = st.file_uploader("ä¸Šä¼  .docx æ–‡ä»¶", type="docx")
    
    if uploaded_word is not None:
        if st.button("å¼€å§‹éªŒè¯ (Word)", key="btn_word"):
            doc = docx.Document(uploaded_word)
            full_text = []
            for para in doc.paragraphs:
                if len(para.text) > 20: # å¿½ç•¥å¤ªçŸ­çš„è¡Œ
                    full_text.append(para.text)
            
            st.write(f"æå–åˆ° {len(full_text)} ä¸ªå¯èƒ½çš„å¼•ç”¨æ®µè½ã€‚")
            
            results_txt = "=== æ–‡çŒ®éªŒè¯æŠ¥å‘Š ===\n\n"
            progress_bar = st.progress(0)
            
            for i, line in enumerate(full_text):
                progress_bar.progress((i + 1) / len(full_text))
                
                # å‡è®¾è¿™ä¸€è¡Œå°±æ˜¯åŒ…å«æ ‡é¢˜çš„å¼•ç”¨
                # ç®€å•æ¸…æ´—ï¼šå»æ‰å‰é¢çš„ [1] ä¹‹ç±»çš„
                clean_line = line.split(']')[-1].strip() if ']' in line else line
                # æˆªå–å¤§æ¦‚çš„æ ‡é¢˜ä½ç½®ï¼ˆè¿™é‡Œæ¯”è¾ƒç²—ç³™ï¼Œä¾èµ– Semantic Scholar çš„å¼ºæœç´¢èƒ½åŠ›ï¼‰
                
                paper = search_semantic_scholar(clean_line)
                
                if paper:
                    score = fuzz.token_set_ratio(clean_line, paper['title'])
                    if score > 80:
                         results_txt += f"[âœ… çœŸ] åŸæ–‡: {line[:50]}...\n      -> åŒ¹é…: {paper['title']} ({paper['year']})\n\n"
                    else:
                         results_txt += f"[âš ï¸ å­˜ç–‘] åŸæ–‡: {line[:50]}...\n      -> æœåˆ°æœ€æ¥è¿‘: {paper['title']} (ç›¸ä¼¼åº¦ä½)\n\n"
                else:
                    results_txt += f"[âŒ å¹»è§‰/æœªæ‰¾åˆ°] {line}\n\n"
            
            st.text_area("éªŒè¯æŠ¥å‘Š", results_txt, height=300)
            st.download_button(
                label="ğŸ“¥ ä¸‹è½½éªŒè¯æŠ¥å‘Š (.txt)",
                data=results_txt,
                file_name="verification_report.txt",
                mime="text/plain"
            )
