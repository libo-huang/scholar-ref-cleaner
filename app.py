import streamlit as st
import bibtexparser
from bibtexparser.bwriter import BibTexWriter
from bibtexparser.bibdatabase import BibDatabase
import requests
from scholarly import scholarly, ProxyGenerator
from thefuzz import fuzz
import time
import docx
import random

# --- é¡µé¢é…ç½® ---
st.set_page_config(page_title="AIæ–‡çŒ®å¼ºåŠ›æ¸…æ´—æœº (GSä¼˜å…ˆç‰ˆ)", page_icon="ğŸ§¬", layout="wide")

st.title("ğŸ§¬ Scholar Ref Cleaner (Google Scholar First)")
st.markdown("""
**ç­–ç•¥é€»è¾‘**ï¼š
1. ğŸ¥‡ **ä¼˜å…ˆå°è¯• Google Scholar** (æƒå¨ï¼Œä½†å®¹æ˜“è§¦å‘éªŒè¯ç /å°é”)ã€‚
2. ğŸ¥ˆ **è‡ªåŠ¨é™çº§ Semantic Scholar** (è‹¥ Google å¤±è´¥æˆ–è¢«å°ï¼Œè‡ªåŠ¨åˆ‡æ¢æ­¤æºï¼Œé€Ÿåº¦å¿«ä¸”ç¨³å®š)ã€‚
3. ğŸ¥‰ **è‡ªåŠ¨æ¯”å¯¹ä¸æ¸…æ´—** (ä¿®æ­£å¹´ä»½ã€ä½œè€…ã€æœŸåˆŠ)ã€‚

*æ³¨ï¼šéƒ¨ç½²åœ¨äº‘ç«¯æœåŠ¡å™¨æ—¶ï¼ŒGoogle Scholar ææ˜“è§¦å‘åçˆ¬æœºåˆ¶ï¼Œæ­¤æ—¶ä¼šè‡ªåŠ¨åˆ‡æ¢åˆ° Semantic Scholarã€‚*
""")

# --- å…¨å±€å˜é‡ ---
# ç”¨äºè®°å½• Google Scholar æ˜¯å¦å·²ç»æŒ‚äº†ï¼Œå¦‚æœæŒ‚äº†åç»­ç›´æ¥è·³è¿‡ï¼ŒèŠ‚çœæ—¶é—´
if 'gs_blocked' not in st.session_state:
    st.session_state.gs_blocked = False

# --- æ ¸å¿ƒå‡½æ•°ï¼šGoogle Scholar ---
def search_google_scholar(query_title):
    """
    å°è¯•ä» Google Scholar è·å–æ•°æ®
    è¿”å›: (paper_data_dict, success_bool)
    """
    if st.session_state.gs_blocked:
        return None, False

    try:
        # éšæœºä¼‘çœ ï¼Œé™ä½å°å·é£é™©
        time.sleep(random.uniform(1.0, 3.0))
        
        # æœç´¢
        search_query = scholarly.search_pubs(query_title)
        paper = next(search_query) # è·å–ç¬¬ä¸€ä¸ªç»“æœï¼Œå¦‚æœæ²¡æœ‰ä¼šæŠ›å‡º StopIteration
        
        # æå–æ•°æ® (Scholarly è¿”å›çš„æ ¼å¼æ¯”è¾ƒæ·±ï¼Œéœ€è¦æå–)
        bib = paper.get('bib', {})
        
        # ç®€å•çš„æ ¼å¼æ ‡å‡†åŒ–
        result = {
            'title': bib.get('title'),
            'year': bib.get('pub_year'),
            'author': " and ".join(bib.get('author', [])), # GSè¿”å›çš„æ˜¯åˆ—è¡¨ï¼Œè½¬ä¸ºBibTeXå­—ç¬¦ä¸²
            'journal': bib.get('venue'),
            'url': paper.get('pub_url'),
            'source': 'Google Scholar ğŸŸ¢'
        }
        return result, True

    except StopIteration:
        # æ²¡æœåˆ°
        return None, False
    except Exception as e:
        # é‡åˆ°éªŒè¯ç ã€ç½‘ç»œé”™è¯¯ç­‰
        # print(f"GS Error: {e}") # è°ƒè¯•ç”¨
        return None, False

# --- æ ¸å¿ƒå‡½æ•°ï¼šSemantic Scholar ---
def search_semantic_scholar(query_title):
    """
    ä½œä¸ºå¤‡ç”¨æ•°æ®æº
    """
    base_url = "https://api.semanticscholar.org/graph/v1/paper/search"
    params = {
        "query": query_title,
        "limit": 1,
        "fields": "title,authors,year,venue,externalIds,url"
    }
    try:
        # é¿å… API é€Ÿç‡é™åˆ¶
        time.sleep(0.5) 
        response = requests.get(base_url, params=params, timeout=5)
        if response.status_code == 200:
            data = response.json()
            if data['total'] > 0:
                item = data['data'][0]
                
                # æ ¼å¼æ ‡å‡†åŒ–
                author_list = [a['name'] for a in item.get('authors', [])]
                result = {
                    'title': item.get('title'),
                    'year': item.get('year'),
                    'author': " and ".join(author_list),
                    'journal': item.get('venue'),
                    'url': item.get('url'),
                    'source': 'Semantic Scholar ğŸ”µ'
                }
                return result, True
    except Exception as e:
        return None, False
    return None, False

# --- ç»Ÿä¸€è°ƒåº¦å‡½æ•° ---
def unified_search(query_title):
    """
    çº§è”æœç´¢ï¼šGS -> SS
    """
    # 1. å°è¯• Google Scholar
    if not st.session_state.gs_blocked:
        res, found = search_google_scholar(query_title)
        if found:
            return res
        else:
            # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œæˆ–è€…æ˜¯è¢«å°äº†å¯¼è‡´æ²¡ç»“æœï¼Œè¿™é‡Œæ— æ³•ç²¾ç¡®åŒºåˆ†
            # ä¸ºäº†ç¨³å¥ï¼Œåªè¦ GS æ²¡è¿”å›æœ‰æ•ˆæ•°æ®ï¼Œå°±å» SS æŸ¥
            pass
    
    # 2. å°è¯• Semantic Scholar (Fallback)
    res, found = search_semantic_scholar(query_title)
    if found:
        return res
        
    return None

# --- å¤„ç†é€»è¾‘ ---
def process_bib_entry(entry):
    original_title = entry.get('title', '').replace('{', '').replace('}', '').replace('\n', ' ')
    if not original_title:
        return entry, "è·³è¿‡ (æ— æ ‡é¢˜)", 0, "None"

    # è°ƒç”¨ç»Ÿä¸€æœç´¢
    paper = unified_search(original_title)
    
    if not paper:
        entry['note'] = "âš ï¸ NOT FOUND / HALLUCINATION"
        return entry, "âŒ æœªæ‰¾åˆ° (å¯èƒ½æ˜¯å¹»è§‰)", 0, "None"

    # æ¯”å¯¹æ ‡é¢˜ç›¸ä¼¼åº¦
    real_title = paper.get('title', '')
    similarity = fuzz.ratio(original_title.lower(), real_title.lower())

    status = ""
    source_tag = paper['source']
    
    if similarity > 80:
        # åŒ¹é…æˆåŠŸï¼Œè¦†ç›–æ•°æ®
        entry['title'] = real_title
        if paper.get('year'): entry['year'] = str(paper['year'])
        if paper.get('author'): entry['author'] = paper['author']
        if paper.get('journal'): entry['journal'] = paper['journal']
        
        entry['note'] = f"Verified by {paper['source']}"
        status = f"âœ… å·²ä¿®æ­£ ({similarity}%)"
    else:
        entry['note'] = f"â“ Low Confidence (Match: {real_title})"
        status = f"âš ï¸ å­˜ç–‘ (å·®å¼‚å¤§: {real_title})"

    return entry, status, similarity, source_tag

# --- ç•Œé¢é€»è¾‘ ---

tab1, tab2 = st.tabs(["BibTeX æ–‡ä»¶å¤„ç†", "Word/æ–‡æœ¬å¤„ç†"])

# === TAB 1: BibTeX ===
with tab1:
    uploaded_file = st.file_uploader("ä¸Šä¼  .bib æ–‡ä»¶", type="bib")
    
    if uploaded_file is not None:
        bib_database = bibtexparser.load(uploaded_file)
        st.info(f"å…±åŠ è½½ {len(bib_database.entries)} æ¡æ–‡çŒ®ã€‚å¦‚æœæ–‡çŒ®è¾ƒå¤šï¼ŒGoogle Scholar å¯èƒ½ä¼šå˜æ…¢ã€‚")
        
        if st.button("å¼€å§‹æ¸…æ´— (BibTeX)", key="btn_bib"):
            progress_bar = st.progress(0)
            log_area = st.empty()
            
            cleaned_entries = []
            results_data = []

            for i, entry in enumerate(bib_database.entries):
                progress_bar.progress((i + 1) / len(bib_database.entries))
                
                title_preview = entry.get('title', 'Unknown')[:30] + "..."
                log_area.text(f"å¤„ç†ä¸­ [{i+1}/{len(bib_database.entries)}]: {title_preview}")
                
                new_entry, status, score, source = process_bib_entry(entry)
                cleaned_entries.append(new_entry)
                
                results_data.append({
                    "åŸæ ‡é¢˜": entry.get('title')[:30]+"...",
                    "æ•°æ®æº": source,
                    "çŠ¶æ€": status,
                    "ä¿®æ­£å¹´ä»½": new_entry.get('year')
                })

            st.success("å¤„ç†å®Œæˆï¼è¯·æŸ¥çœ‹ä¸‹æ–¹åˆ—è¡¨ç¡®è®¤æ•°æ®æºã€‚")
            st.dataframe(results_data)
            
            db = BibDatabase()
            db.entries = cleaned_entries
            writer = BibTexWriter()
            cleaned_bib_str = writer.write(db)
            
            st.download_button("ğŸ“¥ ä¸‹è½½æ¸…æ´—åçš„ .bib", cleaned_bib_str, "cleaned_gs_priority.bib")

# === TAB 2: Word/Text ===
with tab2:
    st.markdown("ä¸Šä¼  Word æ–‡æ¡£ï¼Œå°†é€è¡Œæå–æ–‡æœ¬ï¼Œä¼˜å…ˆå» Google Scholar éªŒè¯æ˜¯å¦å­˜åœ¨ã€‚")
    uploaded_word = st.file_uploader("ä¸Šä¼  .docx æ–‡ä»¶", type="docx")
    
    if uploaded_word is not None:
        if st.button("å¼€å§‹éªŒè¯ (Word)", key="btn_word"):
            doc = docx.Document(uploaded_word)
            full_text = [p.text for p in doc.paragraphs if len(p.text) > 20]
            
            st.write(f"æå–åˆ° {len(full_text)} ä¸ªæ®µè½ï¼Œå¼€å§‹éªŒè¯...")
            
            report_lines = []
            progress_bar = st.progress(0)
            
            for i, line in enumerate(full_text):
                progress_bar.progress((i + 1) / len(full_text))
                
                # ç®€å•æ¸…æ´—
                clean_line = line.split(']')[-1].strip() if ']' in line else line
                
                paper = unified_search(clean_line)
                
                report_lines.append(f"åŸæ–‡: {line[:60]}...")
                if paper:
                    score = fuzz.token_set_ratio(clean_line, paper['title'])
                    if score > 80:
                         report_lines.append(f"   [{paper['source']}] âœ… åŒ¹é…: {paper['title']} ({paper.get('year')})")
                    else:
                         report_lines.append(f"   [{paper['source']}] âš ï¸ å­˜ç–‘: {paper['title']}")
                else:
                    report_lines.append("   [âŒ] æœªæ‰¾åˆ°/å¹»è§‰")
                report_lines.append("-" * 30)
            
            result_text = "\n".join(report_lines)
            st.text_area("éªŒè¯æŠ¥å‘Š", result_text, height=400)
            st.download_button("ğŸ“¥ ä¸‹è½½æŠ¥å‘Š", result_text, "verification_report.txt")
