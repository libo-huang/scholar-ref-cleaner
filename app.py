import streamlit as st
import bibtexparser
from bibtexparser.bwriter import BibTexWriter
from bibtexparser.bibdatabase import BibDatabase
from scholarly import scholarly
import requests
import docx
from thefuzz import fuzz
import time
import random
import io
import datetime

# ==========================================
# 0. é…ç½®ä¸å¤šè¯­è¨€å­—å…¸ / Configuration & i18n
# ==========================================

st.set_page_config(page_title="Scholar Ref Cleaner", page_icon="ğŸ“", layout="wide")

LANG_DICT = {
    "CN": {
        "title": "ğŸ“ å­¦æœ¯æ–‡çŒ® AI å¹»è§‰æ¸…æ´—æœº",
        "subtitle": """
            **ğŸ›¡ï¸ AI å‚è€ƒæ–‡çŒ®è‡ªåŠ¨æ¸…æ´—æœº** \\
            ä¸Šä¼  BibTeX æˆ– Wordï¼Œç³»ç»Ÿå°†è‡ªåŠ¨æ£€ç´¢å…¨çƒå­¦æœ¯æ•°æ®åº“ï¼Œä¸ºæ‚¨æ‰§è¡Œâ€œä¸‰æ­¥èµ°â€æ¸…æ´—ï¼š
            * **éªŒçœŸ**ï¼šå®ƒæ˜¯çœŸçš„å—ï¼Ÿ(æ£€æµ‹æ˜¯å¦å­˜åœ¨)
            * **çº é”™**ï¼šå®ƒæ˜¯å¯¹çš„å—ï¼Ÿ(ä¿®æ­£å…ƒæ•°æ®)
            * **æŠ¥å‘Š**ï¼šä¸‹è½½æ¸…æ´—åçš„å®Œç¾å¼•ç”¨æ ¼å¼ã€‚
            """,
        "sidebar_title": "è®¾ç½® / Settings",
        "lang_select": "è¯­è¨€ / Language",
        "source_priority": "å½“å‰æ•°æ®æºä¼˜å…ˆçº§ï¼š",
        "instr_title": "ğŸ“– ä½¿ç”¨è¯´æ˜",
        "instr_text": """
        1. **ä¸Šä¼ æ–‡ä»¶**ï¼šæ”¯æŒ .bib (æ¨è), .docx (Word), .txtã€‚
        2. **æ¸…æ´—é€»è¾‘**ï¼š
           - **Step 1**: å°è¯• Google Scholar (æœ€å…¨)ã€‚
           - **Step 2**: å¤±è´¥åˆ™åˆ‡æ¢ Semantic Scholar (å…è´¹)ã€‚
           - **Step 3**: å†æ¬¡å¤±è´¥åˆ™å°è¯• Crossrefã€‚
        3. **ç»“æœ**ï¼š
           - ç›¸ä¼¼åº¦ > 85%ï¼šè‡ªåŠ¨ä¿®æ­£å…ƒæ•°æ®ã€‚
           - ç›¸ä¼¼åº¦ < 50%ï¼šæ ‡è®°ä¸ºâ€œå¹»è§‰/ä¸å­˜åœ¨â€ã€‚
        """,
        "upload_label": "ä¸Šä¼ å‚è€ƒæ–‡çŒ®æ–‡ä»¶",
        "btn_start": "ğŸš€ å¼€å§‹æ¸…æ´— / Start Cleaning",
        "col_original": "åŸæ ‡é¢˜",
        "col_status": "çŠ¶æ€",
        "col_source": "æ¥æº",
        "download_bib": "ğŸ“¥ ä¸‹è½½æ¸…æ´—åçš„ .bib",
        "download_report": "ğŸ“¥ ä¸‹è½½éªŒè¯æŠ¥å‘Š (.txt)",
        "warn_gs": "âš ï¸ æ³¨æ„ï¼šGoogle Scholar ææ˜“å°é”äº‘ç«¯ IPã€‚å¦‚æœå¤„ç†å˜æ…¢ï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨åˆ‡æ¢æºï¼Œè¯·è€å¿ƒç­‰å¾…ã€‚",
        "tab_bib": "BibTeX æ¨¡å¼",
        "tab_doc": "Word/æ–‡æœ¬ æ¨¡å¼",
        "stat_progress": "å¤„ç†è¿›åº¦",
        "stat_verified": "æˆåŠŸéªŒè¯",
        "stat_eta": "é¢„è®¡å‰©ä½™æ—¶é—´",
        "log_title": "ğŸ“œ å®æ—¶å¤„ç†æ—¥å¿—",
        "done_msg": "ğŸ‰ æ¸…æ´—å®Œæˆï¼"
    },
    "EN": {
        "title": "ğŸ“ Scholar Ref Cleaner",
        "subtitle": """
            **ğŸ›¡ï¸ AI Reference Auto-Cleaner**\\
            Upload your BibTeX or Word file. We auto-verify against global academic databases in three steps:
            * **Verify**: Is it real? (Existence Check)
            * **Correct**: Is it accurate? (Metadata Auto-Fix)
            * **Report**: Download your perfectly cleaned citations.
            """,
        "sidebar_title": "Settings",
        "lang_select": "Language",
        "source_priority": "Data Source Priority:",
        "instr_title": "ğŸ“– Instructions",
        "instr_text": """
        1. **Upload**: Supports .bib (Recommended), .docx, .txt.
        2. **Logic**:
           - **Step 1**: Try Google Scholar.
           - **Step 2**: Fallback to Semantic Scholar.
           - **Step 3**: Fallback to Crossref.
        3. **Output**:
           - Similarity > 85%: Auto-correct metadata.
           - Similarity < 50%: Flagged as Hallucination.
        """,
        "upload_label": "Upload Reference File",
        "btn_start": "ğŸš€ Start Cleaning",
        "col_original": "Original Title",
        "col_status": "Status",
        "col_source": "Source",
        "download_bib": "ğŸ“¥ Download Cleaned .bib",
        "download_report": "ğŸ“¥ Download Report (.txt)",
        "warn_gs": "âš ï¸ Note: Google Scholar blocks cloud IPs easily. System auto-switches if blocked.",
        "tab_bib": "BibTeX Mode",
        "tab_doc": "Word/Text Mode",
        "stat_progress": "Progress",
        "stat_verified": "Verified",
        "stat_eta": "Est. Time Remaining",
        "log_title": "ğŸ“œ Live Log",
        "done_msg": "ğŸ‰ All Done!"
    }
}

if 'lang' not in st.session_state:
    st.session_state['lang'] = 'CN'

with st.sidebar:
    st.header("âš™ï¸ " + LANG_DICT[st.session_state['lang']]['sidebar_title'])
    lang_choice = st.radio("Language", ["ä¸­æ–‡", "English"], index=0 if st.session_state['lang']=='CN' else 1)
    if lang_choice == "ä¸­æ–‡": st.session_state['lang'] = 'CN'
    else: st.session_state['lang'] = 'EN'
    st.info(f"**{LANG_DICT[st.session_state['lang']]['source_priority']}**\n\n1. Google Scholar\n2. Semantic Scholar\n3. Crossref")

T = LANG_DICT[st.session_state['lang']]

# ==========================================
# 1. æ ¸å¿ƒæœç´¢é€»è¾‘ / Core Logic
# ==========================================

def search_google_scholar(query):
    try:
        search_query = scholarly.search_pubs(query)
        result = next(search_query)
        return {
            'title': result['bib'].get('title'),
            'year': result['bib'].get('pub_year'),
            'author': " and ".join(result['bib'].get('author', [])),
            'journal': result['bib'].get('venue'),
            'source': 'Google Scholar'
        }
    except Exception:
        return None

def search_semantic_scholar(query):
    url = "https://api.semanticscholar.org/graph/v1/paper/search"
    params = {"query": query, "limit": 1, "fields": "title,authors,year,venue"}
    try:
        r = requests.get(url, params=params, timeout=5)
        if r.status_code == 200 and r.json()['total'] > 0:
            data = r.json()['data'][0]
            authors = [a['name'] for a in data.get('authors', [])]
            return {
                'title': data.get('title'),
                'year': data.get('year'),
                'author': " and ".join(authors),
                'journal': data.get('venue'),
                'source': 'Semantic Scholar'
            }
    except:
        return None
    return None

def search_crossref(query):
    url = "https://api.crossref.org/works"
    params = {"query.bibliographic": query, "rows": 1}
    try:
        r = requests.get(url, params=params, timeout=5)
        if r.status_code == 200:
            items = r.json()['message']['items']
            if items:
                item = items[0]
                year = item['published-print']['date-parts'][0][0] if 'published-print' in item else None
                authors = [f"{a.get('given','')} {a.get('family','')}" for a in item.get('author', [])]
                return {
                    'title': item.get('title', [''])[0],
                    'year': year,
                    'author': " and ".join(authors),
                    'journal': item.get('container-title', [''])[0],
                    'source': 'Crossref'
                }
    except:
        return None
    return None

def waterfall_search(query):
    # ç¨å¾®éšæœºå»¶è¿Ÿï¼Œæ¨¡æ‹Ÿäººç±»è¡Œä¸ºï¼Œä¹Ÿæ–¹ä¾¿è®¡ç®— ETA
    time.sleep(random.uniform(0.8, 1.5)) 
    
    res = search_google_scholar(query)
    if res: return res

    res = search_semantic_scholar(query)
    if res: return res

    res = search_crossref(query)
    if res: return res

    return None

def format_eta(seconds):
    """å°†ç§’æ•°è½¬æ¢ä¸ºäººç±»å¯è¯»æ ¼å¼"""
    if seconds < 60:
        return f"{int(seconds)}s"
    else:
        m, s = divmod(int(seconds), 60)
        return f"{m}m {s}s"

# ==========================================
# 2. ç•Œé¢æ„å»º / UI Builder
# ==========================================

st.title(T['title'])
st.markdown(T['subtitle'])

with st.expander(T['instr_title'], expanded=False):
    st.markdown(T['instr_text'])
    st.warning(T['warn_gs'])

tab1, tab2 = st.tabs([T['tab_bib'], T['tab_doc']])

# --- TAB 1: BibTeX ---
with tab1:
    uploaded_bib = st.file_uploader(T['upload_label'] + " (.bib)", type="bib", key="bib_up")
    
    if uploaded_bib:
        bib_db = bibtexparser.load(uploaded_bib)
        total_items = len(bib_db.entries)
        st.info(f"ğŸ“„ Detected {total_items} entries.")
        
        if st.button(T['btn_start'], key="btn_bib"):
            cleaned_entries = []
            report_data = []
            verified_count = 0
            
            # --- åˆå§‹åŒ– UI å ä½ç¬¦ ---
            # 1. é¡¶éƒ¨æ•°æ®çœ‹æ¿
            dashboard = st.empty()
            # 2. è¿›åº¦æ¡
            prog_bar = st.progress(0)
            # 3. å½“å‰æ­£åœ¨å¤„ç†çš„æ–‡æœ¬
            status_text = st.empty()
            # 4. å®æ—¶æ—¥å¿—åŒºåŸŸ
            log_container = st.expander(T['log_title'], expanded=True)
            
            start_time = time.time()
            
            for i, entry in enumerate(bib_db.entries):
                original_title = entry.get('title', '').replace('{','').replace('}','').replace('\n',' ')
                
                # --- æ›´æ–°æ—¶é—´ä¸ ETA ---
                elapsed_time = time.time() - start_time
                if i > 0:
                    avg_time = elapsed_time / i
                    remaining_time = avg_time * (total_items - i)
                    eta_str = format_eta(remaining_time)
                else:
                    eta_str = "Calculating..."

                # --- æ›´æ–° UI é¢æ¿ ---
                with dashboard.container():
                    c1, c2, c3 = st.columns(3)
                    c1.metric(T['stat_progress'], f"{i + 1} / {total_items}")
                    c2.metric(T['stat_verified'], f"{verified_count}", delta_color="normal")
                    c3.metric(T['stat_eta'], eta_str)
                
                prog_bar.progress((i + 1) / total_items)
                status_text.caption(f"ğŸ” Searching: **{original_title[:50]}...**")

                if not original_title:
                    continue

                # --- æ‰§è¡Œæ ¸å¿ƒé€»è¾‘ ---
                found_data = waterfall_search(original_title)
                
                row = {
                    T['col_original']: original_title,
                    T['col_status']: "âŒ Not Found",
                    T['col_source']: "-"
                }

                log_msg = ""
                if found_data:
                    sim = fuzz.ratio(original_title.lower(), found_data['title'].lower())
                    row[T['col_source']] = found_data['source']
                    
                    if sim > 85:
                        row[T['col_status']] = f"âœ… Verified ({sim}%)"
                        entry['title'] = found_data['title']
                        if found_data['year']: entry['year'] = str(found_data['year'])
                        if found_data['author']: entry['author'] = found_data['author']
                        if found_data['journal']: entry['journal'] = found_data['journal']
                        entry['note'] = f"Verified by {found_data['source']}"
                        
                        verified_count += 1
                        log_msg = f"âœ… Verified: {found_data['title'][:30]}..."
                    elif sim > 50:
                        row[T['col_status']] = f"âš ï¸ Ambiguous ({sim}%)"
                        entry['note'] = f"Ambiguous match: {found_data['title']}"
                        log_msg = f"âš ï¸ Ambiguous: {found_data['title'][:30]}..."
                    else:
                        row[T['col_status']] = f"âŒ Hallucination?"
                        entry['note'] = "Potential Hallucination"
                        log_msg = f"âŒ Hallucination: {original_title[:30]}..."
                else:
                    entry['note'] = "Not Found in any DB"
                    log_msg = f"âŒ Not Found: {original_title[:30]}..."

                # --- å†™å…¥å®æ—¶æ—¥å¿— ---
                log_container.text(f"[{i+1}] {log_msg}")

                cleaned_entries.append(entry)
                report_data.append(row)
            
            # --- å®Œæˆ ---
            status_text.empty()
            st.success(T['done_msg'])
            st.balloons()
            
            st.dataframe(report_data, use_container_width=True)
            
            # ä¸‹è½½
            db = BibDatabase()
            db.entries = cleaned_entries
            writer = BibTexWriter()
            st.download_button(T['download_bib'], writer.write(db), "cleaned.bib", "text/plain")

# --- TAB 2: Word/Text ---
with tab2:
    uploaded_doc = st.file_uploader(T['upload_label'] + " (.docx, .txt)", type=['docx', 'txt'], key="doc_up")
    
    if uploaded_doc and st.button(T['btn_start'], key="btn_doc"):
        lines = []
        if uploaded_doc.name.endswith('.docx'):
            doc = docx.Document(uploaded_doc)
            lines = [p.text for p in doc.paragraphs if len(p.text) > 20]
        else:
            stringio = io.StringIO(uploaded_doc.getvalue().decode("utf-8"))
            lines = [l.strip() for l in stringio.readlines() if len(l) > 20]
            
        total_items = len(lines)
        report_txt = "=== Validation Report ===\n\n"
        verified_count = 0
        
        # --- UI Initialization ---
        dashboard = st.empty()
        prog_bar = st.progress(0)
        status_text = st.empty()
        log_container = st.expander(T['log_title'], expanded=True)
        
        start_time = time.time()
        
        for i, line in enumerate(lines):
            # ETA Calc
            elapsed_time = time.time() - start_time
            if i > 0:
                avg_time = elapsed_time / i
                remaining_time = avg_time * (total_items - i)
                eta_str = format_eta(remaining_time)
            else:
                eta_str = "Calculating..."

            with dashboard.container():
                c1, c2, c3 = st.columns(3)
                c1.metric(T['stat_progress'], f"{i + 1} / {total_items}")
                c2.metric(T['stat_verified'], f"{verified_count}")
                c3.metric(T['stat_eta'], eta_str)
            
            prog_bar.progress((i + 1) / total_items)
            
            query_text = line
            if "]" in query_text[:5]: 
                query_text = query_text.split("]", 1)[1].strip()
            
            status_text.caption(f"ğŸ” Checking: **{query_text[:50]}...**")
            
            found_data = waterfall_search(query_text)
            
            report_txt += f"Original: {line}\n"
            log_msg = ""
            if found_data:
                sim = fuzz.partial_ratio(query_text.lower(), found_data['title'].lower())
                if sim > 80:
                    verified_count += 1
                    report_txt += f"âœ… Match ({found_data['source']}): {found_data['title']} ({found_data['year']})\n"
                    log_msg = f"âœ… Match: {found_data['title'][:30]}..."
                else:
                    report_txt += f"âš ï¸ Low Confidence: Found '{found_data['title']}'\n"
                    log_msg = f"âš ï¸ Low Conf: {found_data['title'][:30]}..."
            else:
                report_txt += "âŒ Not Found (Likely Hallucination)\n"
                log_msg = f"âŒ Not Found"
            
            report_txt += "-"*30 + "\n"
            log_container.text(f"[{i+1}] {log_msg}")
            
        st.success(T['done_msg'])
        st.text_area("Final Report", report_txt, height=300)
        st.download_button(T['download_report'], report_txt, "report.txt", "text/plain")