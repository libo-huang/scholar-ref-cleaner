import streamlit as st
import bibtexparser
from bibtexparser.bwriter import BibTexWriter
from bibtexparser.bibdatabase import BibDatabase
from scholarly import scholarly
import requests
import docx
from thefuzz import fuzz
import time
import random

# ==========================================
# 0. é…ç½®ä¸å¤šè¯­è¨€å­—å…¸ / Configuration & i18n
# ==========================================

st.set_page_config(page_title="Scholar Ref Cleaner Pro", page_icon="ğŸ“", layout="wide")

LANG_DICT = {
    "CN": {
        "title": "ğŸ“ å­¦æœ¯æ–‡çŒ® AI å¹»è§‰æ¸…æ´—æœº (Proç‰ˆ)",
        "subtitle": "ä¼˜å…ˆä½¿ç”¨ Google Scholarï¼Œè‡ªåŠ¨é™çº§è‡³ Semantic Scholar å’Œ Crossrefã€‚",
        "sidebar_title": "è®¾ç½® / Settings",
        "lang_select": "è¯­è¨€ / Language",
        "source_priority": "å½“å‰æ•°æ®æºä¼˜å…ˆçº§ï¼š",
        "instr_title": "ğŸ“– ä½¿ç”¨è¯´æ˜",
        "instr_text": """
        1. **ä¸Šä¼ æ–‡ä»¶**ï¼šæ”¯æŒ .bib (æ¨è), .docx (Word), .txtã€‚
        2. **æ¸…æ´—é€»è¾‘**ï¼š
           - **Step 1**: å°è¯• Google Scholar (æœ€å…¨ï¼Œä½†å®¹æ˜“è§¦å‘éªŒè¯ç )ã€‚
           - **Step 2**: å¤±è´¥åˆ™åˆ‡æ¢ Semantic Scholar (å…è´¹ï¼Œç¨³å®š)ã€‚
           - **Step 3**: å†æ¬¡å¤±è´¥åˆ™å°è¯• Crossref (å‡ºç‰ˆå•†å®˜æ–¹æ•°æ®)ã€‚
        3. **ç»“æœ**ï¼š
           - ç›¸ä¼¼åº¦ > 85%ï¼šè‡ªåŠ¨ä¿®æ­£å…ƒæ•°æ®ã€‚
           - ç›¸ä¼¼åº¦ < 50%ï¼šæ ‡è®°ä¸ºâ€œå¹»è§‰/ä¸å­˜åœ¨â€ã€‚
        """,
        "upload_label": "ä¸Šä¼ å‚è€ƒæ–‡çŒ®æ–‡ä»¶",
        "btn_start": "å¼€å§‹æ¸…æ´— / Start Cleaning",
        "status_processing": "æ­£åœ¨å¤„ç†",
        "col_original": "åŸæ ‡é¢˜",
        "col_status": "çŠ¶æ€",
        "col_source": "æ¥æº",
        "download_bib": "ğŸ“¥ ä¸‹è½½æ¸…æ´—åçš„ .bib",
        "download_report": "ğŸ“¥ ä¸‹è½½éªŒè¯æŠ¥å‘Š (.txt)",
        "warn_gs": "âš ï¸ æ³¨æ„ï¼šGoogle Scholar åœ¨äº‘ç«¯éƒ¨ç½²ææ˜“è¢«å°é” IPã€‚å¦‚æœå¤„ç†é€Ÿåº¦å˜æ…¢æˆ–æŠ¥é”™ï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨åˆ‡æ¢åˆ°åç»­æ•°æ®æºï¼Œè¯·è€å¿ƒç­‰å¾…ã€‚",
        "tab_bib": "BibTeX æ¨¡å¼",
        "tab_doc": "Word/æ–‡æœ¬ æ¨¡å¼"
    },
    "EN": {
        "title": "ğŸ“ Scholar Ref Cleaner Pro",
        "subtitle": "Prioritizes Google Scholar, cascades to Semantic Scholar and Crossref.",
        "sidebar_title": "Settings",
        "lang_select": "Language",
        "source_priority": "Data Source Priority:",
        "instr_title": "ğŸ“– Instructions",
        "instr_text": """
        1. **Upload**: Supports .bib (Recommended), .docx, .txt.
        2. **Logic**:
           - **Step 1**: Try Google Scholar (Best coverage, strict rate limits).
           - **Step 2**: Fallback to Semantic Scholar (Stable, Free).
           - **Step 3**: Fallback to Crossref (Official Publisher Data).
        3. **Output**:
           - Similarity > 85%: Auto-correct metadata.
           - Similarity < 50%: Flagged as Hallucination.
        """,
        "upload_label": "Upload Reference File",
        "btn_start": "Start Cleaning",
        "status_processing": "Processing",
        "col_original": "Original Title",
        "col_status": "Status",
        "col_source": "Source",
        "download_bib": "ğŸ“¥ Download Cleaned .bib",
        "download_report": "ğŸ“¥ Download Report (.txt)",
        "warn_gs": "âš ï¸ Note: Google Scholar blocks cloud IPs easily. The system will auto-switch to other sources if GS fails.",
        "tab_bib": "BibTeX Mode",
        "tab_doc": "Word/Text Mode"
    }
}

# åˆå§‹åŒ– Session State
if 'lang' not in st.session_state:
    st.session_state['lang'] = 'CN'

# ä¾§è¾¹æ 
with st.sidebar:
    st.header("âš™ï¸ " + LANG_DICT[st.session_state['lang']]['sidebar_title'])
    lang_choice = st.radio("Language", ["ä¸­æ–‡", "English"], index=0 if st.session_state['lang']=='CN' else 1)
    if lang_choice == "ä¸­æ–‡": st.session_state['lang'] = 'CN'
    else: st.session_state['lang'] = 'EN'
    
    st.info(f"**{LANG_DICT[st.session_state['lang']]['source_priority']}**\n\n1. Google Scholar\n2. Semantic Scholar\n3. Crossref")

T = LANG_DICT[st.session_state['lang']]

# ==========================================
# 1. æ ¸å¿ƒæœç´¢é€»è¾‘ (Waterfall) / Core Logic
# ==========================================

def search_google_scholar(query):
    """å°è¯• Google Scholar"""
    try:
        search_query = scholarly.search_pubs(query)
        result = next(search_query) # è·å–ç¬¬ä¸€ä¸ªç»“æœ
        return {
            'title': result['bib'].get('title'),
            'year': result['bib'].get('pub_year'),
            'author': " and ".join(result['bib'].get('author', [])),
            'journal': result['bib'].get('venue'),
            'source': 'Google Scholar'
        }
    except Exception:
        return None

def search_semantic_scholar(query):
    """å°è¯• Semantic Scholar"""
    url = "https://api.semanticscholar.org/graph/v1/paper/search"
    params = {"query": query, "limit": 1, "fields": "title,authors,year,venue"}
    try:
        r = requests.get(url, params=params, timeout=5)
        if r.status_code == 200 and r.json()['total'] > 0:
            data = r.json()['data'][0]
            authors = [a['name'] for a in data.get('authors', [])]
            return {
                'title': data.get('title'),
                'year': data.get('year'),
                'author': " and ".join(authors),
                'journal': data.get('venue'),
                'source': 'Semantic Scholar'
            }
    except:
        return None
    return None

def search_crossref(query):
    """å°è¯• Crossref"""
    url = "https://api.crossref.org/works"
    params = {"query.bibliographic": query, "rows": 1}
    try:
        r = requests.get(url, params=params, timeout=5)
        if r.status_code == 200:
            items = r.json()['message']['items']
            if items:
                item = items[0]
                # Crossref è¿”å›çš„æ—¥æœŸæ¯”è¾ƒå¤æ‚
                year = item['published-print']['date-parts'][0][0] if 'published-print' in item else None
                authors = [f"{a.get('given','')} {a.get('family','')}" for a in item.get('author', [])]
                return {
                    'title': item.get('title', [''])[0],
                    'year': year,
                    'author': " and ".join(authors),
                    'journal': item.get('container-title', [''])[0],
                    'source': 'Crossref'
                }
    except:
        return None
    return None

def waterfall_search(query):
    """ç€‘å¸ƒæµæœç´¢æ§åˆ¶å™¨"""
    # 1. Google Scholar (åŠ å»¶è¿Ÿé˜²æ­¢ç§’å°)
    time.sleep(random.uniform(1, 2)) 
    res = search_google_scholar(query)
    if res: return res

    # 2. Semantic Scholar
    time.sleep(0.5)
    res = search_semantic_scholar(query)
    if res: return res

    # 3. Crossref
    time.sleep(0.5)
    res = search_crossref(query)
    if res: return res

    return None

# ==========================================
# 2. ç•Œé¢æ„å»º / UI Builder
# ==========================================

st.title(T['title'])
st.markdown(T['subtitle'])

with st.expander(T['instr_title'], expanded=True):
    st.markdown(T['instr_text'])
    st.warning(T['warn_gs'])

tab1, tab2 = st.tabs([T['tab_bib'], T['tab_doc']])

# --- TAB 1: BibTeX ---
with tab1:
    uploaded_bib = st.file_uploader(T['upload_label'] + " (.bib)", type="bib", key="bib_up")
    
    if uploaded_bib and st.button(T['btn_start'], key="btn_bib"):
        bib_db = bibtexparser.load(uploaded_bib)
        cleaned_entries = []
        report_data = []
        
        progress = st.progress(0)
        status_text = st.empty()
        
        total = len(bib_db.entries)
        for i, entry in enumerate(bib_db.entries):
            progress.progress((i + 1) / total)
            original_title = entry.get('title', '').replace('{','').replace('}','').replace('\n',' ')
            
            if not original_title:
                continue

            status_text.text(f"{T['status_processing']}: {original_title[:40]}...")
            
            # æ‰§è¡Œæœç´¢
            found_data = waterfall_search(original_title)
            
            row = {
                T['col_original']: original_title,
                T['col_status']: "âŒ Not Found",
                T['col_source']: "-"
            }

            if found_data:
                # è®¡ç®—ç›¸ä¼¼åº¦
                sim = fuzz.ratio(original_title.lower(), found_data['title'].lower())
                row[T['col_source']] = found_data['source']
                
                if sim > 85:
                    row[T['col_status']] = f"âœ… Verified ({sim}%)"
                    # æ›´æ–° Bib æ•°æ®
                    entry['title'] = found_data['title']
                    if found_data['year']: entry['year'] = str(found_data['year'])
                    if found_data['author']: entry['author'] = found_data['author']
                    if found_data['journal']: entry['journal'] = found_data['journal']
                    entry['note'] = f"Verified by {found_data['source']}"
                elif sim > 50:
                    row[T['col_status']] = f"âš ï¸ Ambiguous ({sim}%)"
                    entry['note'] = f"Ambiguous match: {found_data['title']}"
                else:
                    row[T['col_status']] = f"âŒ Hallucination?"
                    entry['note'] = "Potential Hallucination"
            else:
                entry['note'] = "Not Found in any DB"

            cleaned_entries.append(entry)
            report_data.append(row)
            
        st.success("Done!")
        st.dataframe(report_data)
        
        # ä¸‹è½½ Bib
        db = BibDatabase()
        db.entries = cleaned_entries
        writer = BibTexWriter()
        st.download_button(T['download_bib'], writer.write(db), "cleaned.bib", "text/plain")

# --- TAB 2: Word/Text ---
with tab2:
    uploaded_doc = st.file_uploader(T['upload_label'] + " (.docx, .txt)", type=['docx', 'txt'], key="doc_up")
    
    if uploaded_doc and st.button(T['btn_start'], key="btn_doc"):
        # è¯»å–æ–‡æœ¬
        lines = []
        if uploaded_doc.name.endswith('.docx'):
            doc = docx.Document(uploaded_doc)
            lines = [p.text for p in doc.paragraphs if len(p.text) > 20] # å¿½ç•¥çŸ­è¡Œ
        else:
            stringio = io.StringIO(uploaded_doc.getvalue().decode("utf-8"))
            lines = [l.strip() for l in stringio.readlines() if len(l) > 20]
            
        report_txt = "=== Validation Report ===\n\n"
        progress = st.progress(0)
        status_text = st.empty()
        
        for i, line in enumerate(lines):
            progress.progress((i + 1) / len(lines))
            # ç®€å•çš„æ¸…ç†ï¼Œå‡è®¾æ¯è¡Œæ˜¯ä¸€ä¸ªå¼•ç”¨
            query_text = line
            # å¦‚æœæœ‰[1]è¿™ç§ç¼–å·ï¼Œå°è¯•å»æ‰
            if "]" in query_text[:5]: 
                query_text = query_text.split("]", 1)[1].strip()
            
            status_text.text(f"{T['status_processing']}: {query_text[:30]}...")
            
            found_data = waterfall_search(query_text)
            
            report_txt += f"Original: {line}\n"
            if found_data:
                sim = fuzz.partial_ratio(query_text.lower(), found_data['title'].lower())
                if sim > 80:
                    report_txt += f"âœ… Match ({found_data['source']}): {found_data['title']} ({found_data['year']})\n"
                else:
                    report_txt += f"âš ï¸ Low Confidence ({found_data['source']}): Found '{found_data['title']}'\n"
            else:
                report_txt += "âŒ Not Found in any database (Likely Hallucination)\n"
            report_txt += "-"*30 + "\n"
            
        st.success("Done!")
        st.text_area("Report", report_txt, height=400)
        st.download_button(T['download_report'], report_txt, "report.txt", "text/plain")
