import streamlit as st
import bibtexparser
from bibtexparser.bwriter import BibTexWriter
from bibtexparser.bibdatabase import BibDatabase
from scholarly import scholarly, ProxyGenerator
import requests
from thefuzz import fuzz
import docx
import time
import pandas as pd
import io

# --- é¡µé¢è®¾ç½® ---
st.set_page_config(page_title="AIæ–‡çŒ®è¶…çº§æ¸…æ´—æœº", page_icon="ğŸ§¬", layout="wide")

# --- å…¨å±€çŠ¶æ€ç®¡ç† (ç”¨äºè®°å½• GS æ˜¯å¦è¢«å°é”) ---
if 'gs_fail_count' not in st.session_state:
    st.session_state.gs_fail_count = 0
if 'gs_blocked' not in st.session_state:
    st.session_state.gs_blocked = False

# --- æ ¸å¿ƒæœç´¢å‡½æ•°ç¾¤ ---

def search_google_scholar(title):
    """
    ä¼˜å…ˆçº§ 1: Google Scholar
    æ³¨æ„ï¼šææ˜“è§¦å‘éªŒè¯ç ï¼Œä»…ä½œä¸ºé¦–é€‰å°è¯•
    """
    if st.session_state.gs_blocked:
        return None, "Blocked"

    try:
        # å¢åŠ éšæœºå»¶è¿Ÿï¼Œå‡å°‘å°é”æ¦‚ç‡
        time.sleep(2) 
        search_query = scholarly.search_pubs(title)
        result = next(search_query) # è·å–ç¬¬ä¸€ä¸ªç»“æœ
        
        # æå–å…³é”®ä¿¡æ¯
        bib = result['bib']
        data = {
            'title': bib.get('title'),
            'year': bib.get('pub_year', ''),
            'author': " and ".join(bib.get('author', [])),
            'journal': bib.get('venue', ''),
            'source': 'Google Scholar'
        }
        return data, "Success"
    except StopIteration:
        return None, "Not Found"
    except Exception as e:
        # è®°å½•å¤±è´¥æ¬¡æ•°ï¼Œè¿ç»­å¤±è´¥3æ¬¡åˆ™ç†”æ–­
        st.session_state.gs_fail_count += 1
        if st.session_state.gs_fail_count >= 3:
            st.session_state.gs_blocked = True
        return None, "Error/Blocked"

def search_semantic_scholar(title):
    """
    ä¼˜å…ˆçº§ 2: Semantic Scholar
    ç¨³å®šã€å…è´¹ã€é€Ÿåº¦å¿«
    """
    try:
        time.sleep(0.5)
        url = "https://api.semanticscholar.org/graph/v1/paper/search"
        params = {"query": title, "limit": 1, "fields": "title,authors,year,venue"}
        r = requests.get(url, params=params, timeout=5)
        if r.status_code == 200:
            data = r.json()
            if data['total'] > 0:
                paper = data['data'][0]
                authors = [a['name'] for a in paper.get('authors', [])]
                return {
                    'title': paper.get('title'),
                    'year': paper.get('year', ''),
                    'author': " and ".join(authors),
                    'journal': paper.get('venue', ''),
                    'source': 'Semantic Scholar'
                }, "Success"
    except:
        pass
    return None, "Not Found"

def search_crossref(title):
    """
    ä¼˜å…ˆçº§ 3: Crossref
    å®˜æ–¹æ•°æ®ï¼Œä½†æ¨¡ç³ŠåŒ¹é…èƒ½åŠ›ç¨å¼±
    """
    try:
        time.sleep(0.5)
        url = "https://api.crossref.org/works"
        params = {"query.bibliographic": title, "rows": 1}
        r = requests.get(url, params=params, timeout=5)
        if r.status_code == 200:
            items = r.json()['message']['items']
            if items:
                item = items[0]
                authors = [f"{a.get('given','')} {a.get('family','')}" for a in item.get('author', [])]
                return {
                    'title': item.get('title', [''])[0],
                    'year': item.get('created', {}).get('date-parts', [[None]])[0][0],
                    'author': " and ".join(authors),
                    'journal': item.get('container-title', [''])[0],
                    'source': 'Crossref'
                }, "Success"
    except:
        pass
    return None, "Not Found"

def cascaded_search(original_title):
    """
    ç€‘å¸ƒæµæœç´¢é€»è¾‘ï¼šGS -> SS -> Crossref
    """
    # 1. Try Google Scholar
    res, status = search_google_scholar(original_title)
    if res: return res
    
    # 2. Try Semantic Scholar
    res, status = search_semantic_scholar(original_title)
    if res: return res

    # 3. Try Crossref
    res, status = search_crossref(original_title)
    if res: return res

    return None

# --- æ–‡ä»¶è§£æè¾…åŠ©å‡½æ•° ---

def parse_docx(file):
    doc = docx.Document(file)
    text_list = []
    for p in doc.paragraphs:
        if len(p.text.strip()) > 30: # å¿½ç•¥è¿‡çŸ­çš„è¡Œ
            text_list.append(p.text.strip())
    return text_list

def parse_txt(file):
    stringio = io.StringIO(file.getvalue().decode("utf-8"))
    return [line.strip() for line in stringio.readlines() if len(line.strip()) > 30]

# --- ç•Œé¢ UI ---

st.markdown("""
# ğŸ§¬ AI æ–‡çŒ®è¶…çº§æ¸…æ´—æœº
### æ”¯æŒ BibTeX / Word / TXT | å¤šæºæ ¡å¯¹ (Google Scholar > Semantic > Crossref)
""")

with st.expander("ğŸ“– **ä½¿ç”¨è¯´æ˜ (ç‚¹å‡»å±•å¼€)**", expanded=True):
    st.markdown("""
    1. **åŠŸèƒ½**ï¼šè‡ªåŠ¨æ£€æµ‹ AI ç”Ÿæˆçš„â€œå¹»è§‰â€æ–‡çŒ®ï¼Œä¿®æ­£å¹´ä»½ã€ä½œè€…å’ŒæœŸåˆŠã€‚
    2. **æ•°æ®æºä¼˜å…ˆçº§**ï¼š
       - ğŸ¥‡ **Google Scholar** (æœ€å…¨ï¼Œä½†å®¹æ˜“è¢«åçˆ¬æ‹¦æˆªï¼Œæ‹¦æˆªåè‡ªåŠ¨è·³è¿‡)
       - ğŸ¥ˆ **Semantic Scholar** (ç¨³å®šï¼Œä¸»åŠ›æ•°æ®æº)
       - ğŸ¥‰ **Crossref** (å®˜æ–¹ DOI æ•°æ®ï¼Œæœ€åå…œåº•)
    3. **æ”¯æŒæ ¼å¼**ï¼š
       - **.bib**: è¾“å‡ºä¿®æ­£åçš„ .bib æ–‡ä»¶ï¼Œå¯ç›´æ¥å¯¼å…¥ LaTeXã€‚
       - **.docx / .txt**: é€è¡Œè¯»å–å‚è€ƒæ–‡çŒ®åˆ—è¡¨ï¼Œè¾“å‡ºæ ¡å¯¹æŠ¥å‘Šã€‚
    """)

# --- ä¾§è¾¹æ è®¾ç½® ---
with st.sidebar:
    st.header("âš™ï¸ è®¾ç½®")
    use_gs = st.checkbox("å¯ç”¨ Google Scholar", value=True, help="å¦‚æœä¸å‹¾é€‰ï¼Œå°†ç›´æ¥ä½¿ç”¨ Semantic Scholarï¼Œé€Ÿåº¦æ›´å¿«ä¸”ç¨³å®šã€‚")
    if not use_gs:
        st.session_state.gs_blocked = True
    
    st.info("ğŸ’¡ æç¤ºï¼šWord æ–‡æ¡£è¯·ç¡®ä¿æ¯æ¡å‚è€ƒæ–‡çŒ®å ä¸€è¡Œã€‚")

# --- ä¸»é€»è¾‘åŒº ---
upload_type = st.radio("é€‰æ‹©ä¸Šä¼ æ–‡ä»¶ç±»å‹", ["BibTeX (.bib)", "Wordæ–‡æ¡£ (.docx) / æ–‡æœ¬ (.txt)"], horizontal=True)
uploaded_file = st.file_uploader("ä¸Šä¼ æ–‡ä»¶", type=['bib', 'docx', 'txt'])

if uploaded_file:
    # ------------------ å¤„ç† BIB æ–‡ä»¶ ------------------
    if upload_type == "BibTeX (.bib)" and uploaded_file.name.endswith('.bib'):
        bib_db = bibtexparser.load(uploaded_file)
        st.write(f"ğŸ“Š è¯†åˆ«åˆ° {len(bib_db.entries)} æ¡æ–‡çŒ®")
        
        if st.button("å¼€å§‹æ¸…æ´—", type="primary"):
            cleaned_entries = []
            report_data = []
            
            progress_bar = st.progress(0)
            status_text = st.empty()

            for i, entry in enumerate(bib_db.entries):
                title = entry.get('title', '').replace('{','').replace('}','')
                progress_bar.progress((i + 1) / len(bib_db.entries))
                status_text.text(f"æ­£åœ¨å¤„ç†: {title[:40]}...")

                # æ‰§è¡Œæœç´¢
                valid_data = cascaded_search(title)
                
                if valid_data:
                    # è®¡ç®—ç›¸ä¼¼åº¦
                    sim = fuzz.ratio(title.lower(), valid_data['title'].lower())
                    
                    if sim > 80:
                        # ä¿®æ­£æ•°æ®
                        entry['title'] = valid_data['title']
                        entry['year'] = str(valid_data['year'])
                        entry['author'] = valid_data['author']
                        entry['journal'] = valid_data['journal']
                        entry['note'] = f"Verified by {valid_data['source']}"
                        
                        report_data.append({
                            "åŸæ ‡é¢˜": title,
                            "ç»“æœ": "âœ… ä¿®æ­£",
                            "æ¥æº": valid_data['source'],
                            "ä¿®æ­£åå¹´ä»½": valid_data['year']
                        })
                    else:
                        entry['note'] = f"Low Confidence Match ({valid_data['source']})"
                        report_data.append({"åŸæ ‡é¢˜": title, "ç»“æœ": "âš ï¸ å­˜ç–‘ (æ ‡é¢˜å·®å¼‚å¤§)", "æ¥æº": valid_data['source'], "ä¿®æ­£åå¹´ä»½": "-"})
                else:
                    entry['note'] = "POSSIBLE HALLUCINATION"
                    report_data.append({"åŸæ ‡é¢˜": title, "ç»“æœ": "âŒ æœªæ‰¾åˆ° (å¯èƒ½æ˜¯å¹»è§‰)", "æ¥æº": "-", "ä¿®æ­£åå¹´ä»½": "-"})
                
                cleaned_entries.append(entry)

            # ç»“æœå±•ç¤º
            st.success("å¤„ç†å®Œæˆï¼")
            st.dataframe(pd.DataFrame(report_data))
            
            # ä¸‹è½½ Bib
            db = BibDatabase()
            db.entries = cleaned_entries
            writer = BibTexWriter()
            st.download_button("ğŸ“¥ ä¸‹è½½æ¸…æ´—åçš„ .bib", writer.write(db), file_name="cleaned.bib")

    # ------------------ å¤„ç† Word/Txt æ–‡ä»¶ ------------------
    elif upload_type == "Wordæ–‡æ¡£ (.docx) / æ–‡æœ¬ (.txt)":
        if st.button("å¼€å§‹éªŒè¯", type="primary"):
            # è¯»å–å†…å®¹
            if uploaded_file.name.endswith('.docx'):
                lines = parse_docx(uploaded_file)
            else:
                lines = parse_txt(uploaded_file)
            
            st.write(f"ğŸ“Š æå–åˆ° {len(lines)} è¡Œæ–‡æœ¬")
            
            report_lines = []
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            for i, line in enumerate(lines):
                progress_bar.progress((i + 1) / len(lines))
                # ç®€å•æ¸…æ´—ï¼šå»é™¤å‰é¢çš„ [1] æˆ– 1. 
                clean_query = line.split(']')[-1].strip() if ']' in line else line
                # å†æ¬¡å°è¯•å»é™¤æ•°å­—ç‚¹ 1. 
                if len(clean_query) > 0 and clean_query[0].isdigit():
                    clean_query = clean_query.split('.', 1)[-1].strip()

                status_text.text(f"æ­£åœ¨æœç´¢: {clean_query[:30]}...")
                
                valid_data = cascaded_search(clean_query)
                
                report_lines.append(f"ğŸ”´ åŸæ–‡: {line}")
                if valid_data:
                    sim = fuzz.token_set_ratio(clean_query, valid_data['title'])
                    if sim > 80:
                        report_lines.append(f"ğŸŸ¢ [âœ… çœŸ - {valid_data['source']}]")
                        report_lines.append(f"    -> åŒ¹é…æ ‡é¢˜: {valid_data['title']}")
                        report_lines.append(f"    -> å¹´ä»½: {valid_data['year']} | æœŸåˆŠ: {valid_data['journal']}")
                    else:
                        report_lines.append(f"ğŸŸ¡ [âš ï¸ å­˜ç–‘ - {valid_data['source']}]")
                        report_lines.append(f"    -> æœç´¢ç»“æœ: {valid_data['title']} (ç›¸ä¼¼åº¦ä½)")
                else:
                    report_lines.append("âš« [âŒ æœªæ‰¾åˆ° - å¯èƒ½æ˜¯AIå¹»è§‰]")
                report_lines.append("-" * 50)
            
            result_text = "\n".join(report_lines)
            st.text_area("éªŒè¯æŠ¥å‘Šé¢„è§ˆ", result_text, height=400)
            st.download_button("ğŸ“¥ ä¸‹è½½éªŒè¯æŠ¥å‘Š (.txt)", result_text, file_name="verification_report.txt")
    
    else:
        st.warning("è¯·ä¸Šä¼ ä¸é€‰æ‹©ç±»å‹åŒ¹é…çš„æ–‡ä»¶ï¼")
