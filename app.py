import streamlit as st
import bibtexparser
from bibtexparser.bwriter import BibTexWriter
from bibtexparser.bibdatabase import BibDatabase
import requests
from thefuzz import fuzz
from scholarly import scholarly, ProxyGenerator
import time
import docx
import random

# --- é¡µé¢é…ç½® ---
st.set_page_config(page_title="è¶…çº§æ–‡çŒ®æ¸…æ´—æœº (GSä¼˜å…ˆç‰ˆ)", page_icon="ğŸ§¬", layout="wide")

st.title("ğŸ§¬ Scholar Ref Cleaner (Multi-Source)")
st.markdown("""
**å¤šæºèåˆæ¸…æ´—ç­–ç•¥ï¼š**
1. ğŸ¥‡ **Google Scholar**: ä¼˜å…ˆå°è¯•ï¼ˆæ•°æ®æœ€å‡†ï¼Œä½†æ˜“è¢«äº‘ç«¯å°é”ï¼‰ã€‚
2. ğŸ¥ˆ **Semantic Scholar**: è‡ªåŠ¨é™çº§å¤‡é€‰ï¼ˆé€Ÿåº¦å¿«ï¼ŒAPIç¨³å®šï¼‰ã€‚
3. ğŸ¥‰ **Crossref**: æœ€ç»ˆå…œåº•ï¼ˆå…¨çƒæœ€å¤§DOIæ•°æ®åº“ï¼‰ã€‚
""")

# --- æ ¸å¿ƒæœç´¢æ¨¡å— ---

def search_google_scholar(query_title):
    """ç­–ç•¥1ï¼šå°è¯• Google Scholar"""
    try:
        # éšæœºä¼‘çœ æ¨¡æ‹Ÿäººç±»ï¼Œé˜²æ­¢ç§’å°ï¼ˆä½†åœ¨äº‘ç«¯ä¾ç„¶å¾ˆéš¾å­˜æ´»ï¼‰
        time.sleep(random.uniform(1, 3)) 
        
        search_query = scholarly.search_pubs(query_title)
        # è·å–ç¬¬ä¸€ä¸ªç»“æœï¼Œå¦‚æœæ— ç»“æœä¼šæŠ›å‡º StopIteration
        result = next(search_query) 
        
        # æ ¼å¼åŒ–æ•°æ®ä»¥ç»Ÿä¸€æ ‡å‡†
        bib = result['bib']
        return {
            'title': bib.get('title'),
            'year': bib.get('pub_year'),
            'author': " and ".join(bib.get('author', [])),
            'journal': bib.get('venue'),
            'source': 'Google Scholar'
        }
    except StopIteration:
        return None # æ²¡æœåˆ°
    except Exception as e:
        # åŒ…å«ç½‘ç»œé”™è¯¯ã€éªŒè¯ç æ‹¦æˆªç­‰æ‰€æœ‰å¼‚å¸¸
        print(f"Google Scholar Failed: {e}") 
        return None

def search_semantic_scholar(query_title):
    """ç­–ç•¥2ï¼šå°è¯• Semantic Scholar"""
    base_url = "https://api.semanticscholar.org/graph/v1/paper/search"
    params = {
        "query": query_title,
        "limit": 1,
        "fields": "title,authors,year,venue"
    }
    try:
        # é¿å…è§¦å‘ API é€Ÿç‡é™åˆ¶
        time.sleep(1.0) 
        response = requests.get(base_url, params=params, timeout=5)
        if response.status_code == 200:
            data = response.json()
            if data['total'] > 0:
                paper = data['data'][0]
                # æ ¼å¼åŒ–
                authors = [a['name'] for a in paper.get('authors', [])]
                return {
                    'title': paper.get('title'),
                    'year': paper.get('year'),
                    'author': " and ".join(authors),
                    'journal': paper.get('venue'),
                    'source': 'Semantic Scholar'
                }
    except Exception:
        return None
    return None

def search_crossref(query_title):
    """ç­–ç•¥3ï¼šå°è¯• Crossref (å…œåº•)"""
    base_url = "https://api.crossref.org/works"
    params = {
        "query.bibliographic": query_title,
        "rows": 1
    }
    try:
        response = requests.get(base_url, params=params, timeout=5)
        if response.status_code == 200:
            data = response.json()
            items = data['message']['items']
            if items:
                paper = items[0]
                # æ ¼å¼åŒ–
                title_list = paper.get('title', [])
                title = title_list[0] if title_list else ""
                
                # Crossrefçš„æ—¶é—´æ ¼å¼æ¯”è¾ƒå¤æ‚
                year = ""
                if 'published-print' in paper:
                    year = paper['published-print']['date-parts'][0][0]
                elif 'created' in paper:
                    year = paper['created']['date-parts'][0][0]
                
                authors = []
                if 'author' in paper:
                    for a in paper['author']:
                        authors.append(f"{a.get('given','')} {a.get('family','')}")
                
                container = paper.get('container-title', [])
                journal = container[0] if container else ""

                return {
                    'title': title,
                    'year': str(year),
                    'author': " and ".join(authors),
                    'journal': journal,
                    'source': 'Crossref'
                }
    except Exception:
        return None
    return None

def waterfall_search(query_title):
    """ç€‘å¸ƒæµè°ƒåº¦å™¨"""
    # 1. ä¼˜å…ˆ Google
    res = search_google_scholar(query_title)
    if res: return res
    
    # 2. å¤±è´¥åˆ™ Semantic
    res = search_semantic_scholar(query_title)
    if res: return res
    
    # 3. å¤±è´¥åˆ™ Crossref
    res = search_crossref(query_title)
    if res: return res
    
    return None

def process_bib_entry(entry):
    """å¤„ç†å•ä¸ª Bib æ¡ç›®"""
    original_title = entry.get('title', '').replace('{', '').replace('}', '').replace('\n', ' ')
    if not original_title:
        return entry, "è·³è¿‡ (æ— æ ‡é¢˜)", 0, "None"

    # æ‰§è¡Œç€‘å¸ƒæµæœç´¢
    paper = waterfall_search(original_title)
    
    if not paper:
        entry['note'] = "âš ï¸ NOT FOUND / HALLUCINATION"
        return entry, "âŒ æœªæ‰¾åˆ° (å¯èƒ½æ˜¯å¹»è§‰)", 0, "None"

    # æ¯”å¯¹
    real_title = paper.get('title', '')
    similarity = fuzz.ratio(original_title.lower(), real_title.lower())
    source_used = paper.get('source', 'Unknown')

    status = ""
    if similarity > 80: # ç¨å¾®æ”¾å®½é˜ˆå€¼ï¼Œå› ä¸ºä¸åŒæ•°æ®åº“æ ‡ç‚¹ä¸åŒ
        entry['title'] = real_title
        if paper.get('year'): entry['year'] = str(paper['year'])
        if paper.get('author'): entry['author'] = paper['author']
        if paper.get('journal'): entry['journal'] = paper['journal']
        
        entry['note'] = f"Verified by {source_used}"
        status = f"âœ… å·²ä¿®æ­£ (æº: {source_used})"
    else:
        entry['note'] = f"â“ Low Confidence (Match: {real_title})"
        status = f"âš ï¸ å­˜ç–‘ (æº: {source_used}, å·®å¼‚å¤§)"

    return entry, status, similarity, source_used

# --- ç•Œé¢é€»è¾‘ ---
uploaded_file = st.file_uploader("ä¸Šä¼  .bib æ–‡ä»¶", type="bib")

if uploaded_file is not None:
    bib_database = bibtexparser.load(uploaded_file)
    if st.button("å¼€å§‹å¤šæºæ¸…æ´—"):
        progress_bar = st.progress(0)
        log_area = st.empty()
        cleaned_entries = []
        results_data = []

        for i, entry in enumerate(bib_database.entries):
            progress_bar.progress((i + 1) / len(bib_database.entries))
            log_area.text(f"æ­£åœ¨å¤„ç† [{i+1}/{len(bib_database.entries)}] - æ­£åœ¨è½®è¯¢å„å¤§æ•°æ®åº“...")
            
            new_entry, status, score, source = process_bib_entry(entry)
            cleaned_entries.append(new_entry)
            
            results_data.append({
                "åŸæ ‡é¢˜": entry.get('title')[:30]+"...",
                "çŠ¶æ€": status,
                "æ•°æ®æº": source,
                "ä¿®æ­£å¹´ä»½": new_entry.get('year')
            })

        st.success("å¤„ç†å®Œæˆï¼")
        st.dataframe(results_data)
        
        db = BibDatabase()
        db.entries = cleaned_entries
        writer = BibTexWriter()
        st.download_button("ğŸ“¥ ä¸‹è½½æ¸…æ´—åçš„ .bib", writer.write(db), "cleaned.bib")
